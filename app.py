import streamlit as st
from groq import Groq
import base64

# --- 1. ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡§µ‡§ö ---
try:
    # ‡§Ö‡§¨ ‡§∏‡§ø‡§∞‡•ç‡§´ ‡§è‡§ï ‡§π‡•Ä ‡§ö‡§æ‡§¨‡•Ä ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§π‡•à
    GROQ_K = st.secrets["GROQ_API_KEY"]
    client_groq = Groq(api_key=GROQ_K)
except:
    st.error("‡§≠‡§æ‡§à, Secrets ‡§Æ‡•á‡§Ç GROQ_API_KEY ‡§ö‡•á‡§ï ‡§ï‡§∞‡•ã!")
    st.stop()

# --- 2. Groq ‡§ï‡§æ '‡§¶‡•á‡§ñ‡§®‡•á' ‡§µ‡§æ‡§≤‡§æ ‡§¶‡§ø‡§Æ‡§æ‡§ó ---
def get_groq_vision_response(text, file):
    try:
        # ‡§´‡•ã‡§ü‡•ã ‡§ï‡•ã ‡§¨‡§æ‡§á‡§®‡§∞‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§®‡§æ
        image_data = base64.b64encode(file.read()).decode('utf-8')
        
        # Groq ‡§ï‡§æ ‡§µ‡§ø‡§ú‡§® ‡§Æ‡•â‡§°‡§≤ ‡§á‡§∏‡•ç‡§§‡•á‡§Æ‡§æ‡§≤ ‡§ï‡§∞‡§®‡§æ
        completion = client_groq.chat.completions.create(
            model="llama-3.2-11b-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": text if text else "‡§á‡§∏ ‡§´‡•ã‡§ü‡•ã ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡§æ‡§ì ‡§≠‡§æ‡§à"},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
                        },
                    ],
                }
            ],
            temperature=0.7,
        )
        return completion.choices[0].message.content, "Groq Vision üì∑"
    except Exception as e:
        return f"‡§≠‡§æ‡§à, ‡§ó‡•ç‡§∞‡•â‡§ï ‡§≠‡•Ä ‡§•‡§ï ‡§ó‡§Ø‡§æ ‡§π‡•à: {str(e)}", "Error"

# --- 3. ‡§á‡§Ç‡§ü‡§∞‡§´‡§º‡•á‡§∏ (Gemini 3 Style) ---
st.set_page_config(page_title="Rajaram AI", page_icon="üëë")

st.markdown("""
    <style>
    .stApp { background-color: #131314; color: white; }
    .chat-bubble { padding: 15px; border-radius: 15px; border: 1px solid #3c3f43; margin-bottom: 15px; }
    </style>
    """, unsafe_allow_html=True)

st.title("üëë Rajaram AI")

if "messages" not in st.session_state:
    st.session_state.messages = []

# ‡§ö‡•à‡§ü ‡§¶‡§ø‡§ñ‡§æ‡§®‡§æ
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.write(m["content"])

# --- 4. ‡§ü‡•Ç‡§≤‡•ç‡§∏ ‡§î‡§∞ ‡§ö‡•à‡§ü ‡§¨‡•â‡§ï‡•ç‡§∏ ---
col1, col2 = st.columns([1, 5])
with col1:
    up_file = st.file_uploader("üì∑", type=['png', 'jpg', 'jpeg'], key="cam", label_visibility="collapsed")

with col2:
    prompt = st.chat_input("‡§Ö‡§¨ ‡§ó‡•Ç‡§ó‡§≤ ‡§ï‡§æ ‡§°‡§∞ ‡§®‡§π‡•Ä‡§Ç, ‡§™‡•Ç‡§õ‡•ã ‡§≠‡§æ‡§à...")

if prompt or up_file:
    user_txt = prompt if prompt else "‡§´‡•ã‡§ü‡•ã ‡§¶‡•á‡§ñ‡•ã ‡§≠‡§æ‡§à"
    
    if not st.session_state.messages or st.session_state.messages[-1]["content"] != user_txt:
        st.session_state.messages.append({"role": "user", "content": user_txt})
        with st.chat_message("user"):
            st.write(user_txt)
            if up_file: st.image(up_file, width=200)

        with st.spinner("‡§∞‡§æ‡§ú‡§æ‡§∞‡§æ‡§Æ AI ‡§ï‡•Ä ‡§µ‡§ø‡§ú‡§® ‡§∂‡§ï‡•ç‡§§‡§ø ‡§ï‡§æ‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•à..."):
            if up_file:
                ans, brain = get_groq_vision_response(user_txt, up_file)
            else:
                # ‡§®‡•â‡§∞‡•ç‡§Æ‡§≤ ‡§ö‡•à‡§ü ‡§ï‡•á ‡§≤‡§ø‡§è 70B ‡§µ‡§æ‡§≤‡§æ ‡§¨‡•ú‡§æ ‡§¶‡§ø‡§Æ‡§æ‡§ó
                res = client_groq.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    messages=[{"role": "user", "content": user_txt}]
                )
                ans, brain = res.choices[0].message.content, "Llama 3.3 ‚ö°"
            
            st.session_state.messages.append({"role": "assistant", "content": ans})
            with st.chat_message("assistant"):
                st.write(ans)
                st.caption(f"Active Power: {brain}")
